[{"kind":1,"language":"markdown","value":"## Buildin ML Algorithm","outputs":[]},{"kind":2,"language":"mlsql","value":"include project.`./src/common/mock_data.mlsql`;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"features\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\"containsNull\": true\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"label\",\n\t\t\t\t\"type\": \"double\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t4.4,\n\t\t\t\t2.9,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t4.7,\n\t\t\t\t3.2,\n\t\t\t\t1.3,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"-- create mock validate/test dataset.\nselect vec_dense(features) as features, label as label from mock_data as mock_data;\nselect * from mock_data as mock_data_validate;\nselect * from mock_data as mock_data_test;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"features\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"udt\",\n\t\t\t\t\t\"class\": \"org.apache.spark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"pyClass\": \"pyspark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"sqlType\": {\n\t\t\t\t\t\t\"type\": \"struct\",\n\t\t\t\t\t\t\"fields\": [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"type\",\n\t\t\t\t\t\t\t\t\"type\": \"byte\",\n\t\t\t\t\t\t\t\t\"nullable\": false,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"size\",\n\t\t\t\t\t\t\t\t\"type\": \"integer\",\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"indices\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"integer\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"values\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"label\",\n\t\t\t\t\"type\": \"double\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t4.4,\n\t\t\t\t\t2.9,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t4.7,\n\t\t\t\t\t3.2,\n\t\t\t\t\t1.3,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"!show et;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"algType\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"sparkCompatibility\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"doc\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"docType\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"name\": \"ALSInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"AutoIncrementKeyExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"CacheExt\",\n\t\t\t\"algType\": \"feature engineer\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\\nSQLCacheExt is used to cache/uncache table.\\n\\n```sql\\nrun table as CacheExt.`` where execute=\\\"cache\\\" and isEager=\\\"true\\\";\\n```\\n\\nIf you execute the upper command, then table will be cached immediately, othersise only the second time\\nto use the table you will fetch the table from cache.\\n\\nTo release the table , do like this:\\n\\n```sql\\nrun table as CacheExt.`` where execute=\\\"uncache\\\";\\n```\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"CommunityBasedSimilarityInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ConfusionMatrix\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"CorpusExplainInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DataSourceExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DicOrTableToArray\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Discretizer\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DownloadExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ExternalPythonAlg\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"2.2.x,2.3.x,2.4.x\",\n\t\t\t\"doc\": \"\\n\\nRequirements:\\n\\n1. Conda is installed in your cluster.\\n2. The user who runs StreamingPro cluster has the permission to read/write `/tmp/__mlsql__`.\\n\\nSuppose you run StreamingPro/MLSQL with user named `mlsql`.\\nConda should be installed by `mlsql` and `mlsql` have the permission to read/write `/tmp/__mlsql__`.\\n\\nYou can get code example by:\\n\\n```\\nload modelExample.`PythonAlg` as output;\\n```\\n\\nActually, this doc is also can be get by this command.\\n\\nIf you wanna know what params the PythonAlg have, please use the command following:\\n\\n```\\nload modelParam.`PythonAlg` as output;\\n```\\n\\n     \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"FPGrowth\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"FeatureExtractInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"GBTRegressor\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"GBTs\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"HashTfIdf\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"JDBC\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"JDBCUpdatExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"KMeans\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Kill\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LDA\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"2.3.x\",\n\t\t\t\"doc\": \"\\n <a href=\\\"http://en.wikipedia.org/wiki/LDA\\\">LDA</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n1. To load data\\n```\\nload libsvm.`D:/soucecode/spark-2.3-src/data/mllib/sample_lda_libsvm_data.txt` as data1;\\n```\\n\\n2. To train LDA Model\\n```\\ntrain data1 as LDA.`/tmp/model` where\\n```\\n\\n-- k: number of topics, or number of clustering centers\\n```\\nk=\\\"3\\\"\\n```\\n\\n-- docConcentration: the hyperparameter (Dirichlet distribution parameter) of article distribution must be >1.0. The larger the value is, the smoother the predicted distribution is\\n```\\nand docConcentration=\\\"3.0\\\"\\n```\\n\\n-- topictemperature: the hyperparameter (Dirichlet distribution parameter) of the theme distribution must be >1.0. The larger the value is, the more smooth the distribution can be inferred\\n```\\nand topicConcentration=\\\"3.0\\\"\\n```\\n\\n-- maxIterations: number of iterations, which need to be fully iterated, at least 20 times or more\\n```\\nand maxIter=\\\"100\\\"\\n```\\n\\n-- setSeed: random seed\\n```\\nand seed=\\\"10\\\"\\n```\\n\\n-- checkpointInterval: interval of checkpoints during iteration calculation\\n```\\nand checkpointInterval=\\\"10\\\"\\n```\\n\\n-- optimizer: optimized calculation method currently supports \\\"em\\\" and \\\"online\\\". Em method takes up more memory, and multiple iterations of memory may not be enough to throw a stack exception\\n```\\nand optimizer=\\\"online\\\";\\n```\\n\\n3. register LDA to UDF\\n```\\nregister LDA.`C:/tmp/model` as lda;\\n```\\n\\n4. use LDA udf\\n```\\nselect label,lda(4) topicsMatrix,lda_doc(features) TopicDistribution,lda_topic(label,4) describeTopics from data as result;\\n```\\n\\n5. save result\\n```\\nsave overwrite result as json.`/tmp/result`;\\n```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LSVM\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LinearRegression\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Linear_Regression\\\">Linear Regression</a> learning algorithm for\\n classification.\\n It usually used for prediction/forecasting/error reduction and variation explain.\\n\\n Use \\\"load modelParams.`LinearRegression` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`LinearRegression` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"LinearRegression\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LinearRegressionExt\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Linear_Regression\\\">Linear Regression</a> learning algorithm for\\n classification.\\n It usually used for prediction/forecasting/error reduction and variation explain.\\n\\n Use \\\"load modelParams.`LinearRegression` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`LinearRegression` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"LinearRegression\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LogisticRegression\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Logistic_regression\\\">Logistic Regression</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n  Outputs with more than two values are modeled by multinomial logistic regression and,\\n  if the multiple categories are ordered, by ordinal logistic regression\\n  (for example the proportional odds ordinal logistic mode）\\n\\n Use \\\"load modelParams.`LogisticRegression` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`LogisticRegression` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"LogisticRegression\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Map\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"MapValues\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ModelExplainInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"NaiveBayes\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\\\">Naive Bayes</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n Use \\\"load modelParams.`NaiveBayes` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`NaiveBayes` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"NaiveBayes\\\" as output;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"NormalizeInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PageRank\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PythonAlg\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"2.2.x,2.3.x,2.4.x\",\n\t\t\t\"doc\": \"\\n\\nRequirements:\\n\\n1. Conda is installed in your cluster.\\n2. The user who runs StreamingPro cluster has the permission to read/write `/tmp/__mlsql__`.\\n\\nSuppose you run StreamingPro/MLSQL with user named `mlsql`.\\nConda should be installed by `mlsql` and `mlsql` have the permission to read/write `/tmp/__mlsql__`.\\n\\nYou can get code example by:\\n\\n```\\nload modelExample.`PythonAlg` as output;\\n```\\n\\nActually, this doc is also can be get by this command.\\n\\nIf you wanna know what params the PythonAlg have, please use the command following:\\n\\n```\\nload modelParam.`PythonAlg` as output;\\n```\\n\\n     \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PythonAlgBP\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PythonEnvExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PythonParallelExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RandomForest\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\\n <a href=\\\"http://en.wikipedia.org/wiki/Random_forest\\\">Random Forest</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n Use \\\"load modelParams.`RandomForest` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`RandomForest` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"RandomForest\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RateSampler\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RawSimilarInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ReduceFeaturesInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RepartitionExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RowMatrix\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ScalaScriptUDF\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\\n## Script support\\n\\nScript e.g. Python,Scala nested in MLSQL provides more fine-grained control when doing some ETL tasks, as it allows you\\neasily create SQL function with more powerful language which can do complex logical task.\\n\\nCause the tedious of java's grammar, we will not support java script.\\n\\nBefore use ScriptUDF module, you can use\\n\\n```\\nload modelParams.`ScriptUDF` as output;\\n```\\n\\nto check how to configure this module.\\n\\n### Python UDF Script Example\\n\\n```sql\\n-- using set statement to hold your python script\\n-- Notice that the first parameter of function you defined should be self.\\nset echoFun='''\\n\\ndef apply(self,m):\\n    return m\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be processed more conveniently.\\nload script.`echoFun` as scriptTable;\\n\\n-- register `apply` as UDF named `echoFun`\\nregister ScriptUDF.`scriptTable` as echoFun options\\n-- specify which script you choose\\nand lang=\\\"python\\\"\\n-- As we know python is not strongly typed language, so\\n-- we should manually spcify the return type.\\n-- map(string,string) means a map with key is string type,value also is string type.\\n-- array(string) means a array with string type element.\\n-- nested is support e.g. array(array(map(string,array(string))))\\nand dataType=\\\"map(string,string)\\\"\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect echoFun(map('a','b')) as res from dataTable as output;\\n```\\n\\n### Scala UDF Script Example\\n\\n```sql\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be process more convenient.\\nload script.`plusFun` as scriptTable;\\n\\n-- register `apply` as UDF named `plusFun`\\nregister ScriptUDF.`scriptTable` as plusFun\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as res from dataTable as output;\\n```\\n\\n\\n### Python UDAF Example\\n\\n```sql\\nset plusFun='''\\nfrom org.apache.spark.sql.expressions import MutableAggregationBuffer, UserDefinedAggregateFunction\\nfrom org.apache.spark.sql.types import DataTypes,StructType\\nfrom org.apache.spark.sql import Row\\nimport java.lang.Long as l\\nimport java.lang.Integer as i\\n\\nclass SumAggregation:\\n\\n    def inputSchema(self):\\n        return StructType().add(\\\"a\\\", DataTypes.LongType)\\n\\n    def bufferSchema(self):\\n        return StructType().add(\\\"total\\\", DataTypes.LongType)\\n\\n    def dataType(self):\\n        return DataTypes.LongType\\n\\n    def deterministic(self):\\n        return True\\n\\n    def initialize(self,buffer):\\n        return buffer.update(i(0), l(0))\\n\\n    def update(self,buffer, input):\\n        sum = buffer.getLong(i(0))\\n        newitem = input.getLong(i(0))\\n        buffer.update(i(0), l(sum + newitem))\\n\\n    def merge(self,buffer1, buffer2):\\n        buffer1.update(i(0), l(buffer1.getLong(i(0)) + buffer2.getLong(i(0))))\\n\\n    def evaluate(self,buffer):\\n        return buffer.getLong(i(0))\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\nand lang=\\\"python\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n### Scala UDAF Script Example\\n\\n```sql\\nset plusFun='''\\nimport org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\\nimport org.apache.spark.sql.types._\\nimport org.apache.spark.sql.Row\\nclass SumAggregation extends UserDefinedAggregateFunction with Serializable{\\n    def inputSchema: StructType = new StructType().add(\\\"a\\\", LongType)\\n    def bufferSchema: StructType =  new StructType().add(\\\"total\\\", LongType)\\n    def dataType: DataType = LongType\\n    def deterministic: Boolean = true\\n    def initialize(buffer: MutableAggregationBuffer): Unit = {\\n      buffer.update(0, 0l)\\n    }\\n    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\\n      val sum   = buffer.getLong(0)\\n      val newitem = input.getLong(0)\\n      buffer.update(0, sum + newitem)\\n    }\\n    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\\n      buffer1.update(0, buffer1.getLong(0) + buffer2.getLong(0))\\n    }\\n    def evaluate(buffer: Row): Any = {\\n      buffer.getLong(0)\\n    }\\n}\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n\\n### Some tricks\\n\\nYou can simplify the definition of UDF register like following:\\n\\n```sql\\nregister ScriptUDF.`` as count_board options lang=\\\"python\\\"\\n    and methodName=\\\"apply\\\"\\n    and dataType=\\\"map(string,integer)\\\"\\n    and code='''\\ndef apply(self, s):\\n    from collections import Counter\\n    return dict(Counter(s))\\n    '''\\n;\\n```\\n\\n\\nMulti methods defined onetime is also supported.\\n\\n```sql\\n\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\ndef hello(a:String)={\\n   s\\\"hello: ${a}\\\"\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as plusFun;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as plus, helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\nYou can also define this methods in a class:\\n\\n```sql\\n\\nset plusFun='''\\n\\nclass ScalaScript {\\n    def apply(a:Double,b:Double)={\\n       a + b\\n    }\\n\\n    def hello(a:String)={\\n       s\\\"hello: ${a}\\\"\\n    }\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\nand className=\\\"ScalaScript\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\n\\n\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ScalerInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ScriptUDF\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\\n## Script support\\n\\nScript e.g. Python,Scala nested in MLSQL provides more fine-grained control when doing some ETL tasks, as it allows you\\neasily create SQL function with more powerful language which can do complex logical task.\\n\\nCause the tedious of java's grammar, we will not support java script.\\n\\nBefore use ScriptUDF module, you can use\\n\\n```\\nload modelParams.`ScriptUDF` as output;\\n```\\n\\nto check how to configure this module.\\n\\n### Python UDF Script Example\\n\\n```sql\\n-- using set statement to hold your python script\\n-- Notice that the first parameter of function you defined should be self.\\nset echoFun='''\\n\\ndef apply(self,m):\\n    return m\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be processed more conveniently.\\nload script.`echoFun` as scriptTable;\\n\\n-- register `apply` as UDF named `echoFun`\\nregister ScriptUDF.`scriptTable` as echoFun options\\n-- specify which script you choose\\nand lang=\\\"python\\\"\\n-- As we know python is not strongly typed language, so\\n-- we should manually spcify the return type.\\n-- map(string,string) means a map with key is string type,value also is string type.\\n-- array(string) means a array with string type element.\\n-- nested is support e.g. array(array(map(string,array(string))))\\nand dataType=\\\"map(string,string)\\\"\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect echoFun(map('a','b')) as res from dataTable as output;\\n```\\n\\n### Scala UDF Script Example\\n\\n```sql\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be process more convenient.\\nload script.`plusFun` as scriptTable;\\n\\n-- register `apply` as UDF named `plusFun`\\nregister ScriptUDF.`scriptTable` as plusFun\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as res from dataTable as output;\\n```\\n\\n\\n### Python UDAF Example\\n\\n```sql\\nset plusFun='''\\nfrom org.apache.spark.sql.expressions import MutableAggregationBuffer, UserDefinedAggregateFunction\\nfrom org.apache.spark.sql.types import DataTypes,StructType\\nfrom org.apache.spark.sql import Row\\nimport java.lang.Long as l\\nimport java.lang.Integer as i\\n\\nclass SumAggregation:\\n\\n    def inputSchema(self):\\n        return StructType().add(\\\"a\\\", DataTypes.LongType)\\n\\n    def bufferSchema(self):\\n        return StructType().add(\\\"total\\\", DataTypes.LongType)\\n\\n    def dataType(self):\\n        return DataTypes.LongType\\n\\n    def deterministic(self):\\n        return True\\n\\n    def initialize(self,buffer):\\n        return buffer.update(i(0), l(0))\\n\\n    def update(self,buffer, input):\\n        sum = buffer.getLong(i(0))\\n        newitem = input.getLong(i(0))\\n        buffer.update(i(0), l(sum + newitem))\\n\\n    def merge(self,buffer1, buffer2):\\n        buffer1.update(i(0), l(buffer1.getLong(i(0)) + buffer2.getLong(i(0))))\\n\\n    def evaluate(self,buffer):\\n        return buffer.getLong(i(0))\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\nand lang=\\\"python\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n### Scala UDAF Script Example\\n\\n```sql\\nset plusFun='''\\nimport org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\\nimport org.apache.spark.sql.types._\\nimport org.apache.spark.sql.Row\\nclass SumAggregation extends UserDefinedAggregateFunction with Serializable{\\n    def inputSchema: StructType = new StructType().add(\\\"a\\\", LongType)\\n    def bufferSchema: StructType =  new StructType().add(\\\"total\\\", LongType)\\n    def dataType: DataType = LongType\\n    def deterministic: Boolean = true\\n    def initialize(buffer: MutableAggregationBuffer): Unit = {\\n      buffer.update(0, 0l)\\n    }\\n    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\\n      val sum   = buffer.getLong(0)\\n      val newitem = input.getLong(0)\\n      buffer.update(0, sum + newitem)\\n    }\\n    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\\n      buffer1.update(0, buffer1.getLong(0) + buffer2.getLong(0))\\n    }\\n    def evaluate(buffer: Row): Any = {\\n      buffer.getLong(0)\\n    }\\n}\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n\\n### Some tricks\\n\\nYou can simplify the definition of UDF register like following:\\n\\n```sql\\nregister ScriptUDF.`` as count_board options lang=\\\"python\\\"\\n    and methodName=\\\"apply\\\"\\n    and dataType=\\\"map(string,integer)\\\"\\n    and code='''\\ndef apply(self, s):\\n    from collections import Counter\\n    return dict(Counter(s))\\n    '''\\n;\\n```\\n\\n\\nMulti methods defined onetime is also supported.\\n\\n```sql\\n\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\ndef hello(a:String)={\\n   s\\\"hello: ${a}\\\"\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as plusFun;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as plus, helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\nYou can also define this methods in a class:\\n\\n```sql\\n\\nset plusFun='''\\n\\nclass ScalaScript {\\n    def apply(a:Double,b:Double)={\\n       a + b\\n    }\\n\\n    def hello(a:String)={\\n       s\\\"hello: ${a}\\\"\\n    }\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\nand className=\\\"ScalaScript\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\n\\n\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"SendMessage\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ShowFunctionsExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ShowTableExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"StandardScaler\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"StringIndex\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"TableToMap\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"TfIdf\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"TfIdfInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"TokenAnalysis\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"TokenExtract\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"TreeBuildExt\",\n\t\t\t\"algType\": \"feature engineer\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\\n  TreeBuildExt used to build a tree when you have father - child relationship in some table,\\n  please check the codeExample to see how to use it.\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"UploadFileToServerExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"VecMapInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"WaterMarkInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Word2ArrayInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Word2VecInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Word2vec\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"XGBoostExt\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"2.3.x\",\n\t\t\t\"doc\": \"\\nXGBoostExt is based on [xgboost4j-spark](https://xgboost.readthedocs.io/en/latest/jvm/scaladocs/xgboost4j-spark/index.html).\\n\\nIf you wanna use this module, compile StreamingPro with -Pstreamingpro-xgboost enabled.\\n\\nCheck model params:\\n\\n ```sql\\n load modelExplain.`/tmp/model` where alg=\\\"XGBoostExt\\\" as outout;\\n ```\\n\\nCheck Alg params:\\n\\n```sql\\n load modelParam.`XGBoostExt`  as outout;\\n```\\n\\n     \",\n\t\t\t\"docType\": \"md\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"!show et;\n!lastCommand named ets;\nselect * from ets where name like \"%Random%\" as output;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"algType\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"sparkCompatibility\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"doc\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"docType\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"name\": \"RandomForest\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"2.4.x,3.1.x\",\n\t\t\t\"doc\": \"\\n <a href=\\\"http://en.wikipedia.org/wiki/Random_forest\\\">Random Forest</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n Use \\\"load modelParams.`RandomForest` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`RandomForest` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"RandomForest\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"!show et/RandomForest;\nload modelParams.`RandomForest` as output;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"param\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"description\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"param\": \"evaluateTable\",\n\t\t\t\"description\": \" The table name of test dataset when tranning (undefined)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"keepVersion\",\n\t\t\t\"description\": \" If set true, then every time you run the algorithm, it will generate a new directory to save the model. (default: true)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].cacheNodeIds\",\n\t\t\t\"description\": \" If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].checkpointInterval\",\n\t\t\t\"description\": \" set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].featureSubsetStrategy\",\n\t\t\t\"description\": \" The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].featuresCol\",\n\t\t\t\"description\": \" features column name (default: features)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].impurity\",\n\t\t\t\"description\": \" Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].labelCol\",\n\t\t\t\"description\": \" label column name (default: label)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].maxBins\",\n\t\t\t\"description\": \" Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].maxDepth\",\n\t\t\t\"description\": \" Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].maxMemoryInMB\",\n\t\t\t\"description\": \" Maximum memory in MB allocated to histogram aggregation. (default: 256)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].minInfoGain\",\n\t\t\t\"description\": \" Minimum information gain for a split to be considered at a tree node. (default: 0.0)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].minInstancesPerNode\",\n\t\t\t\"description\": \" Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].numTrees\",\n\t\t\t\"description\": \" Number of trees to train (>= 1) (default: 20)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].predictionCol\",\n\t\t\t\"description\": \" prediction column name (default: prediction)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].probabilityCol\",\n\t\t\t\"description\": \" Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].rawPredictionCol\",\n\t\t\t\"description\": \" raw prediction (a.k.a. confidence) column name (default: rawPrediction)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].seed\",\n\t\t\t\"description\": \" random seed (default: 207336481)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].subsamplingRate\",\n\t\t\t\"description\": \" Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].thresholds\",\n\t\t\t\"description\": \" Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"-- use RandomForest\ntrain mock_data as RandomForest.`/tmp/model` where\nkeepVersion=\"true\" \nand evaluateTable=\"mock_data_validate\"\n\nand `fitParam.0.labelCol`=\"label\"\nand `fitParam.0.featuresCol`=\"features\"\nand `fitParam.0.maxDepth`=\"2\"\n\nand `fitParam.1.featuresCol`=\"features\"\nand `fitParam.1.labelCol`=\"label\"\nand `fitParam.1.maxDepth`=\"10\"\n;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"value\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"name\": \"---------------\",\n\t\t\t\"value\": \"------------------\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"modelPath\",\n\t\t\t\"value\": \"/_model_0/model/1\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"algIndex\",\n\t\t\t\"value\": \"1\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"alg\",\n\t\t\t\"value\": \"org.apache.spark.ml.classification.RandomForestClassifier\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"metrics\",\n\t\t\t\"value\": \"f1:  0.7625000000000001\\nweightedPrecision:  0.8444444444444446\\nweightedRecall:  0.7999999999999999\\naccuracy:  0.8\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"status\",\n\t\t\t\"value\": \"success\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"message\",\n\t\t\t\"value\": \"\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"startTime\",\n\t\t\t\"value\": \"20210905 13:10:11:396\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"endTime\",\n\t\t\t\"value\": \"20210905 13:10:13:515\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"trainParams\",\n\t\t\t\"value\": \"Map(labelCol -> label, featuresCol -> features, maxDepth -> 10)\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"---------------\",\n\t\t\t\"value\": \"------------------\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"modelPath\",\n\t\t\t\"value\": \"/_model_0/model/0\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"algIndex\",\n\t\t\t\"value\": \"0\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"alg\",\n\t\t\t\"value\": \"org.apache.spark.ml.classification.RandomForestClassifier\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"metrics\",\n\t\t\t\"value\": \"f1:  0.7625000000000001\\nweightedPrecision:  0.8444444444444446\\nweightedRecall:  0.7999999999999999\\naccuracy:  0.8\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"status\",\n\t\t\t\"value\": \"success\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"message\",\n\t\t\t\"value\": \"\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"startTime\",\n\t\t\t\"value\": \"20210905 13:10:13:516\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"endTime\",\n\t\t\t\"value\": \"20210905 13:10:14:529\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"trainParams\",\n\t\t\t\"value\": \"Map(maxDepth -> 2, featuresCol -> features, labelCol -> label)\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"predict mock_data_test as RandomForest.`/tmp/model`  as predicted_table;\n-- to display features \n-- select vec_array(features) as features from predicted_table as ouput;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"features\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"udt\",\n\t\t\t\t\t\"class\": \"org.apache.spark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"pyClass\": \"pyspark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"sqlType\": {\n\t\t\t\t\t\t\"type\": \"struct\",\n\t\t\t\t\t\t\"fields\": [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"type\",\n\t\t\t\t\t\t\t\t\"type\": \"byte\",\n\t\t\t\t\t\t\t\t\"nullable\": false,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"size\",\n\t\t\t\t\t\t\t\t\"type\": \"integer\",\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"indices\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"integer\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"values\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"label\",\n\t\t\t\t\"type\": \"double\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"rawPrediction\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"udt\",\n\t\t\t\t\t\"class\": \"org.apache.spark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"pyClass\": \"pyspark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"sqlType\": {\n\t\t\t\t\t\t\"type\": \"struct\",\n\t\t\t\t\t\t\"fields\": [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"type\",\n\t\t\t\t\t\t\t\t\"type\": \"byte\",\n\t\t\t\t\t\t\t\t\"nullable\": false,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"size\",\n\t\t\t\t\t\t\t\t\"type\": \"integer\",\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"indices\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"integer\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"values\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"probability\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"udt\",\n\t\t\t\t\t\"class\": \"org.apache.spark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"pyClass\": \"pyspark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"sqlType\": {\n\t\t\t\t\t\t\"type\": \"struct\",\n\t\t\t\t\t\t\"fields\": [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"type\",\n\t\t\t\t\t\t\t\t\"type\": \"byte\",\n\t\t\t\t\t\t\t\t\"nullable\": false,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"size\",\n\t\t\t\t\t\t\t\t\"type\": \"integer\",\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"indices\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"integer\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"values\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"prediction\",\n\t\t\t\t\"type\": \"double\",\n\t\t\t\t\"nullable\": false,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.362137862137862,\n\t\t\t\t\t5.637862137862137\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7181068931068931,\n\t\t\t\t\t0.28189310689310687\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.362137862137862,\n\t\t\t\t\t5.637862137862137\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7181068931068931,\n\t\t\t\t\t0.28189310689310687\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.362137862137862,\n\t\t\t\t\t5.637862137862137\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7181068931068931,\n\t\t\t\t\t0.28189310689310687\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t4.4,\n\t\t\t\t\t2.9,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t15.791899766899768,\n\t\t\t\t\t4.208100233100232\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7895949883449884,\n\t\t\t\t\t0.21040501165501163\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.362137862137862,\n\t\t\t\t\t5.637862137862137\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7181068931068931,\n\t\t\t\t\t0.28189310689310687\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.362137862137862,\n\t\t\t\t\t5.637862137862137\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7181068931068931,\n\t\t\t\t\t0.28189310689310687\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.362137862137862,\n\t\t\t\t\t5.637862137862137\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7181068931068931,\n\t\t\t\t\t0.28189310689310687\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t4.7,\n\t\t\t\t\t3.2,\n\t\t\t\t\t1.3,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t4.890384615384615,\n\t\t\t\t\t15.109615384615383\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.24451923076923077,\n\t\t\t\t\t0.7554807692307691\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.362137862137862,\n\t\t\t\t\t5.637862137862137\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7181068931068931,\n\t\t\t\t\t0.28189310689310687\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.362137862137862,\n\t\t\t\t\t5.637862137862137\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7181068931068931,\n\t\t\t\t\t0.28189310689310687\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"run predicted_table as ConfusionMatrix.`/tmp/models/model_acc` where \nactualCol=\"label\" and \npredictCol=\"prediction\";\nload parquet.`/tmp/models/model_acc/detail` as output;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"lable\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"value\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"desc\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"PPV\",\n\t\t\t\"value\": \"0.7777777777777778\",\n\t\t\t\"desc\": \"Precision or positive prediction value\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"FN\",\n\t\t\t\"value\": \"2\",\n\t\t\t\"desc\": \"False negative [eqv with miss, Type II error]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"FN\",\n\t\t\t\"value\": \"0\",\n\t\t\t\"desc\": \"False negative [eqv with miss, Type II error]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"PPV\",\n\t\t\t\"value\": \"1.0\",\n\t\t\t\"desc\": \"Precision or positive prediction value\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"FDR\",\n\t\t\t\"value\": \"0.2222222222222222\",\n\t\t\t\"desc\": \"False discovery rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"TPR\",\n\t\t\t\"value\": \"1.0\",\n\t\t\t\"desc\": \"Sensitivity or true positive rate [eqv with hit rate, recall]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"SPC\",\n\t\t\t\"value\": \"0.3333333333333333\",\n\t\t\t\"desc\": \"Specificity or true negative rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"TPR\",\n\t\t\t\"value\": \"0.3333333333333333\",\n\t\t\t\"desc\": \"Sensitivity or true positive rate [eqv with hit rate, recall]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"SPC\",\n\t\t\t\"value\": \"1.0\",\n\t\t\t\"desc\": \"Specificity or true negative rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"TN\",\n\t\t\t\"value\": \"1\",\n\t\t\t\"desc\": \"True negative [eqv with correct rejection]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"FP\",\n\t\t\t\"value\": \"2\",\n\t\t\t\"desc\": \"False positive [eqv with false alarm, Type I error]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"TN\",\n\t\t\t\"value\": \"7\",\n\t\t\t\"desc\": \"True negative [eqv with correct rejection]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"FP\",\n\t\t\t\"value\": \"0\",\n\t\t\t\"desc\": \"False positive [eqv with false alarm, Type I error]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"TP\",\n\t\t\t\"value\": \"7\",\n\t\t\t\"desc\": \"True positive [eqv with hit]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"TP\",\n\t\t\t\"value\": \"1\",\n\t\t\t\"desc\": \"True positive [eqv with hit]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"NPV\",\n\t\t\t\"value\": \"1.0\",\n\t\t\t\"desc\": \"Negative predictive value\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"FPR\",\n\t\t\t\"value\": \"0.6666666666666666\",\n\t\t\t\"desc\": \"Fall-out or false positive rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"NPV\",\n\t\t\t\"value\": \"0.7777777777777778\",\n\t\t\t\"desc\": \"Negative predictive value\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"FPR\",\n\t\t\t\"value\": \"0.0\",\n\t\t\t\"desc\": \"Fall-out or false positive rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"FDR\",\n\t\t\t\"value\": \"0.0\",\n\t\t\t\"desc\": \"False discovery rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"FNR\",\n\t\t\t\"value\": \"0.6666666666666666\",\n\t\t\t\"desc\": \"Miss Rate or False Negative Rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"ACC\",\n\t\t\t\"value\": \"0.8\",\n\t\t\t\"desc\": \"Accuracy\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"FNR\",\n\t\t\t\"value\": \"0.0\",\n\t\t\t\"desc\": \"Miss Rate or False Negative Rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"ACC\",\n\t\t\t\"value\": \"0.8\",\n\t\t\t\"desc\": \"Accuracy\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"register RandomForest.`/tmp/model` as model_predict;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"value\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"name\": \"uid\",\n\t\t\t\"value\": \"rfc_eb07c59f5bd9\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"numFeatures\",\n\t\t\t\"value\": \"4\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"numClasses\",\n\t\t\t\"value\": \"2\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"numTrees\",\n\t\t\t\"value\": \"20\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"treeWeights\",\n\t\t\t\"value\": \"1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"fitParam.[group].featuresCol\",\n\t\t\t\"value\": \"features\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"fitParam.[group].labelCol\",\n\t\t\t\"value\": \"label\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"fitParam.[group].maxDepth\",\n\t\t\t\"value\": \"10\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"select vec_array(model_predict(features)) from mock_data as output;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"UDF:vec_array(UDF(features))\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\"containsNull\": false\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"UDF:vec_array(UDF(features))\": [\n\t\t\t\t0.7181068931068931,\n\t\t\t\t0.28189310689310687\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"UDF:vec_array(UDF(features))\": [\n\t\t\t\t0.7181068931068931,\n\t\t\t\t0.28189310689310687\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"UDF:vec_array(UDF(features))\": [\n\t\t\t\t0.7181068931068931,\n\t\t\t\t0.28189310689310687\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"UDF:vec_array(UDF(features))\": [\n\t\t\t\t0.7895949883449884,\n\t\t\t\t0.21040501165501163\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"UDF:vec_array(UDF(features))\": [\n\t\t\t\t0.7181068931068931,\n\t\t\t\t0.28189310689310687\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"UDF:vec_array(UDF(features))\": [\n\t\t\t\t0.7181068931068931,\n\t\t\t\t0.28189310689310687\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"UDF:vec_array(UDF(features))\": [\n\t\t\t\t0.7181068931068931,\n\t\t\t\t0.28189310689310687\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"UDF:vec_array(UDF(features))\": [\n\t\t\t\t0.24451923076923077,\n\t\t\t\t0.7554807692307691\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"UDF:vec_array(UDF(features))\": [\n\t\t\t\t0.7181068931068931,\n\t\t\t\t0.28189310689310687\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"UDF:vec_array(UDF(features))\": [\n\t\t\t\t0.7181068931068931,\n\t\t\t\t0.28189310689310687\n\t\t\t]\n\t\t}\n\t]\n}"}]}]